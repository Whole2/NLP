{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Chulainn\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing.......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.479 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform data.......\n",
      "training model.......\n",
      "test.......\n",
      "accuracy_score:  0.9933321647677475\n",
      "precision_score:  0.9742960288808664\n",
      "recall_score:  0.9577684718574775\n",
      "f1_score:  0.9659615591109202\n",
      "auc:  0.9774994098877852\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_data(train_file):\n",
    "    target = []\n",
    "    data = []\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line) == 1:\n",
    "                continue\n",
    "            target.append(int(line[0]))\n",
    "            data.append(line[1])\n",
    "    data = list(map(jieba.lcut, data))\n",
    "    data = [\" \".join(d) for d in data]\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def train(cls, data, target, model_path):\n",
    "    cls = cls.fit(data, target)\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(cls, f)\n",
    "\n",
    "def trans(data, matrix_path, stopword_path):\n",
    "    with open(stopword_path, 'r', encoding='utf-8') as fs:\n",
    "        stop_words = [line.strip() for line in fs.readline()]\n",
    "    tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=stop_words)\n",
    "    features = tfidf.fit_transform(data)\n",
    "    with open(matrix_path, 'wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "    return features\n",
    "\n",
    "\n",
    "def load_models(matrix_path, model_path):\n",
    "    tfidf, cls = None, None\n",
    "    if os.path.isfile(model_path):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            cls = pickle.load(f)\n",
    "    if os.path.isfile(matrix_path):\n",
    "        with open(matrix_path, 'rb') as f:\n",
    "            tfidf = pickle.load(f)\n",
    "    return tfidf, cls\n",
    "\n",
    "def test(matrix_path, model_path, data_path, outdir):\n",
    "\n",
    "    curr_time = datetime.datetime.now()\n",
    "    time_str = curr_time.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    out_path = outdir + '/%s/' % time_str\n",
    "    out_file = os.path.join(out_path, \"results.txt\")\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    data, target = get_data(data_path)\n",
    "    tfidf, cls = load_models(matrix_path, model_path)\n",
    "    if tfidf==None or cls==None:\n",
    "        print(\"cannot load models........\")\n",
    "        return\n",
    "\n",
    "    feature = tfidf.transform(data)\n",
    "    predicted = cls.predict(feature)\n",
    "\n",
    "    acc = metrics.accuracy_score(target, predicted)\n",
    "    pre = metrics.precision_score(target, predicted)\n",
    "    recall = metrics.recall_score(target, predicted)\n",
    "    f1 = metrics.f1_score(target, predicted)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predicted)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print(\"accuracy_score: \", acc)\n",
    "    print(\"precision_score: \", pre)\n",
    "    print(\"recall_score: \", recall)\n",
    "    print(\"f1_score: \", f1)\n",
    "    print(\"auc: \", auc)\n",
    "\n",
    "    with open(out_file, 'w', encoding='utf-8') as f:\n",
    "        for label in predicted:\n",
    "            f.write(str(label) + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default='./data/train.txt', help='training data')\n",
    "    parser.add_argument('--test', type=str, default='./data/test.txt', help='test data')\n",
    "    parser.add_argument('--stopwords', type=str, default='./data/hit_stopwords.txt', help='stop words')\n",
    "    parser.add_argument('--model', type=str, default='./model/svm_model.pkl', help='classification model')\n",
    "    parser.add_argument('--matrix', type=str, default='./model/tfidf.pkl', help='tfidf model')\n",
    "    parser.add_argument('--outpath', type=str, default='./results/', help='out path')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    print(\"data processing.......\")\n",
    "    data, target = get_data(args.train)\n",
    "\n",
    "    print(\"transform data.......\")\n",
    "    features = trans(data, args.matrix, args.stopwords)\n",
    "\n",
    "    print(\"training model.......\")\n",
    "    cls = svm.LinearSVC()\n",
    "    train(cls, features, target, args.model)\n",
    "\n",
    "    print(\"test.......\")\n",
    "    test(args.matrix, args.model, args.test, args.outpath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
